{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d482e185-7009-4de0-a17d-c4bd230d3129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/lss/home/dirs/zyongjian/.conda/envs/6119/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_array\n",
    "from scipy.sparse import kron\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy.typing as npt\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from utils import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def ToScipy(M):\n",
    "    N1, N2 = M.size(0), M.size(1)\n",
    "    indices = M.indices().cpu().numpy()\n",
    "    values = M.values().cpu().numpy()\n",
    "    adj = coo_array((values, (indices[0], indices[1])), shape=(N1,N2))\n",
    "    return adj\n",
    "\n",
    "def ScipyKron(X, Y, convert_X=False, convert_Y=False):\n",
    "    if convert_X == True:\n",
    "        X = ToScipy(X.coalesce())\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    if convert_Y == True:\n",
    "        Y = ToScipy(Y.coalesce())\n",
    "    return sparse_mx_to_torch_sparse_tensor(kron(X, Y))\n",
    "    \n",
    "\n",
    "def FixedPointForward(W_list, X, A_list, V, X_list, phi_list, num_iter=1000, tol=1e-6, trasposed_A=False, compute_dphi=False):\n",
    "        \n",
    "        B_list = []\n",
    "        \n",
    "        for i in range(len(X_list)):\n",
    "#             support_1 = torch.spmm(Omega[i], X_list[i])\n",
    "#             support_1 = torch.spmm(torch.transpose(A_list[i], 0, 1), support_1.T).T\n",
    "            support_2 = torch.spmm(V, X_list[i])\n",
    "            #support_2 = torch.spmm(A_list[i], support_2.T).T\n",
    "            B_list.append(  support_2)\n",
    "#             B_list.append(X_list[i])\n",
    "        \n",
    "        \n",
    "#         err = 0\n",
    "        status = 'max itrs reached'\n",
    "#         W = W_list[0]\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "        for iters in range(num_iter):\n",
    "            # WXA\n",
    "            err = 0\n",
    "            X_old = X\n",
    "            for j in range(len(A_list)):\n",
    "                W = W_list[j]\n",
    "                A = A_list[j]\n",
    "#                 At = A if trasposed_A else torch.transpose(A, 0, 1)\n",
    "                B = B_list[j]\n",
    "                phi = phi_list[j]\n",
    "                X_ = W @ X\n",
    "                support = torch.spmm(A, X_.T).T\n",
    "\n",
    "                X = phi(support + B)\n",
    "            err = torch.norm(X - X_old, np.inf)\n",
    "            if err < tol:\n",
    "                status = 'converged'\n",
    "                break\n",
    "\n",
    "        if status == 'max itrs reached':\n",
    "                \n",
    "                print('Forward Not Converge! Error: %3.5f, tol: %3.5f' % (err, tol))\n",
    "        \n",
    "        X_list = []\n",
    "        dphi = []\n",
    "        for j in range(len(A_list)):\n",
    "            W = W_list[j]\n",
    "            A = A_list[j]\n",
    "#             At = A if trasposed_A else torch.transpose(A, 0, 1)\n",
    "            B = B_list[j]\n",
    "            phi = phi_list[j]\n",
    "            X_ = W @ X\n",
    "            support = torch.spmm(A, X_.T).T\n",
    "            X = phi(support + B)\n",
    "            X_list.append(X)\n",
    "            \n",
    "            if compute_dphi:\n",
    "                with torch.enable_grad():\n",
    "                    Z = support + B\n",
    "                    Z.requires_grad_(True)\n",
    "                    tmp = phi(Z)\n",
    "                    dphi.append(torch.autograd.grad(torch.sum(tmp), Z, only_inputs=True)[0])\n",
    "                    \n",
    "    \n",
    "\n",
    "        \n",
    "#         if compute_dphi:\n",
    "#             with torch.enable_grad():\n",
    "#                 for j in range(len(A_list)):\n",
    "# #                     W = W_list[j]\n",
    "#                     A = A_list[j]\n",
    "#                     At = A if trasposed_A else torch.transpose(A, 0, 1)\n",
    "#                     B = B_list[j]\n",
    "#                     X = X_list[j]\n",
    "#                     phi = phi_list[j] \n",
    "                           \n",
    "#                     X_ = W @ X\n",
    "#                     support = torch.spmm(A, X_.T).T\n",
    "#                     Z = support + B\n",
    "#                     Z.requires_grad_(True)\n",
    "#                     X_new = phi(Z)\n",
    "#                     dphi.append(torch.autograd.grad(torch.sum(X_new), Z, only_inputs=True)[0])\n",
    "\n",
    "        return X_list, err, iters, status, dphi\n",
    "    \n",
    "def FixedPointBackward(X, idx, M_list, H_list, phi_list, num_iter=1000, tol=1e-6):\n",
    "    \n",
    "#         err = 0\n",
    "        status = 'max itrs reached'\n",
    "        for iters in range(num_iter):\n",
    "            # WXA\n",
    "            X_old = X\n",
    "            for j in range(len(M_list)):\n",
    "                M = M_list[j]\n",
    "                phi = phi_list[j]\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                if idx==-1:\n",
    "                    tmp = torch.spmm(M, X) + H_list[j]\n",
    "                elif j==idx:\n",
    "                    tmp = torch.spmm(M, X) + H_list[j]\n",
    "                else:\n",
    "                    tmp = torch.spmm(M, X)\n",
    "                X = phi * tmp\n",
    "                \n",
    "            err = torch.norm(X - X_old, np.inf)\n",
    "            X_old = X\n",
    "            if err < tol:\n",
    "                status = 'converged'\n",
    "                break\n",
    "        \n",
    "        if status == 'max itrs reached':\n",
    "                print('Backward Not Converge! Error: %3.5f, tol: %3.5f' % (err, tol))\n",
    "\n",
    "        X_list = []\n",
    "        for j in range(len(M_list)):\n",
    "            M = M_list[j]\n",
    "            phi = phi_list[j]\n",
    "            if idx==-1:\n",
    "                tmp = torch.spmm(M, X) + H_list[j]\n",
    "            elif j==idx:\n",
    "                tmp = torch.spmm(M, X) + H_list[j]\n",
    "            else:\n",
    "                tmp = torch.spmm(M, X)\n",
    "            X = phi * tmp\n",
    "            X_list.append(X)\n",
    "            \n",
    "        return X_list, err, iters, status            \n",
    "\n",
    "class ImpDynGNN(nn.Module):\n",
    "    def __init__(self, num_in, num_hid, num_out, num_node, time_steps, kappa=0.99, phi=F.relu, b_direct=False):\n",
    "        super(ImpDynGNN, self).__init__()\n",
    "        self.i = num_in\n",
    "        self.h = num_hid\n",
    "        self.o = num_out\n",
    "        self.n = num_node\n",
    "        self.t = time_steps\n",
    "        self.k = kappa\n",
    "        self.direct = b_direct\n",
    "\n",
    "        self.phi = [F.relu]*self.t\n",
    "        self.X_0 = Parameter(torch.zeros(self.h, num_node), requires_grad=False)\n",
    "        self.W = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.h)) for i in range(self.t)])\n",
    "#         self.W = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.h))])\n",
    "#         self.Omega = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.i)) for i in range(self.t)])\n",
    "        self.V = Parameter(torch.FloatTensor(self.h, self.i))\n",
    "#         self.linear = nn.Linear(self.h, self.o)\n",
    "        self.classifier = nn.Sequential(\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Linear(self.h, self.h),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Linear(self.h, self.o),\n",
    "                                    # nn.LogSoftmax(dim=1)\n",
    "                                    )\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "#         stdv = 0.01\n",
    "        for i in range(len(self.W)):\n",
    "            stdv = 1. / (math.sqrt(self.W[i].size(1)))\n",
    "            self.W[i].data.uniform_(-stdv, stdv)\n",
    "#             self.Omega[i].data.uniform_(-stdv, stdv)\n",
    "#         stdv = 1. / self.W[0].size(1)\n",
    "#         self.W[0].data.uniform_(-stdv, stdv)\n",
    "        stdv = 1. /self.V.size(1)\n",
    "        self.V.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, X_list, A_list, A_rho, fd_mitr=300):\n",
    "\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = projection_norm_inf(self.W[i], kappa=self.k / A_rho[i])\n",
    "\n",
    "#         self.W[0] = projection_norm_inf(self.W[0], kappa=self.k / min(A_rho))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Z_list, err, iters, status, dphi = FixedPointForward(self.W, self.X_0, \n",
    "                                                              A_list, self.V,\n",
    "                                                              X_list, self.phi,\n",
    "                                                              compute_dphi=True, tol=1e-6)\n",
    "            self.dphi = dphi\n",
    "\n",
    "        \n",
    "#         Z_list = Z_list[-1:] + Z_list[:-1]\n",
    "        Z_list = [Z.requires_grad_(True) for Z in Z_list]\n",
    "#         out_list = [torch.max(Z.T, 1, keepdim=True)[0] for Z in Z_list]\n",
    "        out_list = [self.classifier(Z.T) for Z in Z_list]\n",
    "        return Z_list, out_list\n",
    "    \n",
    "    def backProp(self, Z, X, A, num_iter=600, tol=1e-6):\n",
    "        W = self.W\n",
    "#         Omega = self.Omega\n",
    "        dphi = self.dphi\n",
    "        V = self.V\n",
    "        num_z = Z[0].shape[0]*Z[0].shape[1]\n",
    "        num_w = W[0].shape[0]*W[0].shape[1]\n",
    "        num_v = V.shape[0]*V.shape[1]\n",
    "#         num_omega = Omega[0].shape[0]*Omega[0].shape[1]\n",
    "    \n",
    "        device = Z[0].device\n",
    "        w_grad = torch.zeros(num_z, num_w).to(device)\n",
    "#         o_grad = torch.zeros(num_z, num_omega).to(device)\n",
    "        v_grad = torch.zeros(num_z, num_v).to(device)\n",
    "        \n",
    "        dphi = [torch.flatten(i.T).reshape(-1,1) for i in dphi]\n",
    "    \n",
    "    \n",
    "        M = []\n",
    "        H_hat_v = []\n",
    "        H_hat_w = []\n",
    "#         H_hat_o = []\n",
    "        # E_w = scipy.sparse.eye(W[0].shape[0])\n",
    "        # E_v = scipy.sparse.eye(V.shape[0])\n",
    "        \n",
    "        E_w = torch.eye(W[0].shape[0]).to(device)\n",
    "        E_v = torch.eye(V.shape[0]).to(device)\n",
    "#         E_o = torch.eye(Omega[0].shape[0]).to(device)\n",
    "\n",
    "#         w = W[0]\n",
    "#         wT = torch.transpose(w, 0, 1).contiguous()\n",
    "        for i in range(len(A)):\n",
    "            aa = A[i]\n",
    "            x = X[i]\n",
    "            w = W[i]\n",
    "            if i==0:\n",
    "                z = Z[-1]\n",
    "            else:\n",
    "                z = Z[i-1]\n",
    "                \n",
    "            \n",
    "            \n",
    "            aT = torch.transpose(aa, 0, 1).contiguous()\n",
    "            if x.shape[0] == 1:\n",
    "                xT = x.reshape(-1,1)\n",
    "            else:\n",
    "                xT = torch.transpose(x, 0, 1).contiguous()\n",
    "            \n",
    "            zT = torch.transpose(z, 0, 1).contiguous()\n",
    "            wT = torch.transpose(w, 0, 1).contiguous()\n",
    "            \n",
    "            # H = ToScipy(torch.spmm(aT,zT).to_sparse())\n",
    "            # N = ToScipy(torch.spmm(aT,xT).to_sparse())\n",
    "            H = torch.spmm(aT,zT).to(device)\n",
    "            # N = torch.spmm(aT,xT).to(device)\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            \n",
    "            # M.append(ScipyKron(wT.cpu().numpy(), aa , convert_X=False, convert_Y=True).to(device))\n",
    "            # H_hat_v.append(ScipyKron(xT.cpu().numpy(), E_v).to(device))\n",
    "            # H_hat_w.append(ScipyKron(H, E_w).to(device))\n",
    "            \n",
    "            M.append(torch.kron(wT, aa.to_dense()).to(device))\n",
    "            H_hat_v.append(torch.kron(xT, E_v).to(device))\n",
    "            H_hat_w.append(torch.kron(H, E_w).to(device))\n",
    "#             H_hat_o.append(torch.kron(N, E_o).to(device))\n",
    "    \n",
    "        for i in range(len(W)):\n",
    "            W_grad_list, _, _, status = FixedPointBackward(w_grad, i, M, H_hat_w, dphi, num_iter=num_iter, tol=tol)\n",
    "            \n",
    "#             O_grad_list, _, _, _ = FixedPointBackward(o_grad, i, M, H_hat_o, dphi, num_iter=num_iter, tol=tol)\n",
    "            W_grad, O_grad = 0, 0\n",
    "            for j in range(len(Z)):\n",
    "                if Z[j].grad is None:\n",
    "                    z = torch.zeros_like(Z[j]).reshape(1,-1).to(device)\n",
    "                else:\n",
    "                    z = torch.flatten(Z[j].grad.T).reshape(1,-1)\n",
    "                W_grad += z @ W_grad_list[j]\n",
    "#                 O_grad += z @ O_grad_list[j]\n",
    "                \n",
    "            W[i].grad = W_grad.reshape(W[i].shape[1], W[i].shape[0]).T\n",
    "#             Omega[i].grad = O_grad.reshape(Omega[i].shape[1], Omega[i].shape[0]).T\n",
    "        \n",
    "        V_grad_list, _, _, _ = FixedPointBackward(v_grad, -1, M, H_hat_v, dphi, num_iter=num_iter, tol=tol)\n",
    "        V_grad = 0\n",
    "        for j in range(len(Z)):\n",
    "                if Z[j].grad is None:\n",
    "                    z = torch.zeros_like(Z[j]).reshape(1,-1).to(device)\n",
    "                else:\n",
    "                    z = torch.flatten(Z[j].grad.T).reshape(1,-1)\n",
    "                V_grad += z @ V_grad_list[j]\n",
    "        V.grad = V_grad.reshape(V.shape[1], V.shape[0]).T\n",
    "        \n",
    "        \n",
    "R\"\"\"\n",
    "\"\"\"\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PeMS(object):\n",
    "    R\"\"\"\n",
    "    PeMS dataset.\n",
    "    \"\"\"\n",
    "    #\n",
    "    DISTRICT: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dirname: str,\n",
    "        /,\n",
    "        *,\n",
    "        aug_minutes: bool, aug_weekdays: bool,\n",
    "    ) -> None:\n",
    "        R\"\"\"\n",
    "        Initialize the class.\n",
    "        \"\"\"\n",
    "        #\n",
    "        self.from_raw(dirname)\n",
    "        self.sanitize_edge()\n",
    "\n",
    "        #\n",
    "        self.raw_nodes: npt.NDArray[np.generic]\n",
    "\n",
    "        # Augment global features by exact timestamps.\n",
    "        # Gap between different steps are 5 minutes, and we use hour as\n",
    "        # timestamp unit.\n",
    "        (num_timestamps, num_nodes, _) = self.raw_nodes.shape\n",
    "        self.timestamps = np.arange(num_timestamps) * 5.0 / 24.0\n",
    "\n",
    "        # Augment node features by minutes.\n",
    "        # Gap between different steps are 5 minutes.\n",
    "        if aug_minutes:\n",
    "            #\n",
    "            (num_timestamps, num_nodes, _) = self.raw_nodes.shape\n",
    "            num_day_minutes = 60 // 5 * 24\n",
    "            num_days = (\n",
    "                int(np.ceil(float(num_timestamps) / float(num_day_minutes)))\n",
    "            )\n",
    "            day_minutes = np.arange(num_day_minutes) * 5\n",
    "            minutes = np.tile(day_minutes, num_days)[:num_timestamps]\n",
    "            minutes = minutes.astype(self.raw_nodes.dtype)\n",
    "            minutes = np.reshape(minutes, (num_timestamps, 1, 1))\n",
    "            minutes = np.tile(minutes, (1, num_nodes, 1))\n",
    "            self.raw_nodes = np.concatenate([self.raw_nodes, minutes], 2)\n",
    "\n",
    "        # Augment node features by weekdays.\n",
    "        # Gap between different steps are 5 minutes.\n",
    "        if aug_weekdays:\n",
    "            #\n",
    "            (num_timestamps, num_nodes, _) = self.raw_nodes.shape\n",
    "            num_day_minutes = 60 // 5 * 24\n",
    "            num_week_minutes = num_day_minutes * 7\n",
    "            num_weeks = (\n",
    "                int(np.ceil(float(num_timestamps) / float(num_week_minutes)))\n",
    "            )\n",
    "            weekdays = np.repeat(np.arange(7), num_day_minutes)\n",
    "            weekdays = np.tile(weekdays, num_weeks)[:num_timestamps]\n",
    "            weekdays = weekdays.astype(self.raw_nodes.dtype)\n",
    "            weekdays = np.reshape(weekdays, (num_timestamps, 1, 1))\n",
    "            weekdays = np.tile(weekdays, (1, num_nodes, 1))\n",
    "            self.raw_nodes = np.concatenate([self.raw_nodes, weekdays], 2)\n",
    "\n",
    "    def from_raw(self, dirname: str, /) -> None:\n",
    "        R\"\"\"\n",
    "        Load from raw data.\n",
    "        \"\"\"\n",
    "        #\n",
    "        file_edges = \"distance.csv\"\n",
    "        file_nodes = \"pems{:s}.npz\".format(self.DISTRICT)\n",
    "        raw_edges = pd.read_csv(os.path.join(dirname, file_edges))\n",
    "        self.raw_edge_srcs = raw_edges[\"from\"].to_numpy()\n",
    "        self.raw_edge_dsts = raw_edges[\"to\"].to_numpy()\n",
    "        self.raw_edge_feats = raw_edges[\"cost\"].to_numpy()\n",
    "        self.raw_nodes = np.load(os.path.join(dirname, file_nodes))[\"data\"]\n",
    "\n",
    "    def sanitize_edge(self, /) -> None:\n",
    "        R\"\"\"\n",
    "        Santiize edge data.\n",
    "        \"\"\"\n",
    "        #\n",
    "        collects: Dict[Tuple[int, int], List[float]]\n",
    "\n",
    "        # Remove dirty duplications.\n",
    "        # Duplications are same undirected connections regardless of edge\n",
    "        # weights (edge weights should be the same for those duplications).\n",
    "        collects = {}\n",
    "        for (src, dst, feat) in (\n",
    "            zip(self.raw_edge_srcs, self.raw_edge_dsts, self.raw_edge_feats)\n",
    "        ):\n",
    "            key = (src.item(), dst.item())\n",
    "            key = (min(key), max(key))\n",
    "            val = feat.item()\n",
    "            if key in collects:\n",
    "                collects[key].append(val)\n",
    "            else:\n",
    "                collects[key] = [val]\n",
    "        edge_srcs_buf = []\n",
    "        edge_dsts_buf = []\n",
    "        edge_feats_buf = []\n",
    "        for ((src, dst), feats) in collects.items():\n",
    "            #\n",
    "            edge_srcs_buf.append(src)\n",
    "            edge_dsts_buf.append(dst)\n",
    "            edge_feats_buf.append(sum(feats) / len(feats))\n",
    "            if min(feats) != max(feats):\n",
    "                # UNEXPECT:\n",
    "                # Duplicate edges have different edge features.\n",
    "                raise NotImplementedError(\n",
    "                    \"PeMS duplicate edges have different edge features.\",\n",
    "                )\n",
    "        self.edge_srcs = np.array(edge_srcs_buf)\n",
    "        self.edge_dsts = np.array(edge_dsts_buf)\n",
    "        self.edge_feats = np.array(edge_feats_buf)\n",
    "        self.edge_hetero = False\n",
    "        self.edge_symmetric = True\n",
    "\n",
    "        #\n",
    "        if not np.all(self.edge_feats > 0):\n",
    "            # UNEXPECT:\n",
    "            # Edge features as weights must be positive.\n",
    "            raise NotImplementedError(\n",
    "                \"PeMS{:s} edge weights is not all-positive.\"\n",
    "                .format(self.DISTRICT),\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "class PeMS04(PeMS):\n",
    "    R\"\"\"\n",
    "    PeMS (district 4) dataset.\n",
    "    \"\"\"\n",
    "    #\n",
    "    DISTRICT = \"04\"\n",
    "\n",
    "\n",
    "class PeMS08(PeMS):\n",
    "    R\"\"\"\n",
    "    PeMS (district 8) dataset.\n",
    "    \"\"\"\n",
    "    #\n",
    "    DISTRICT = \"08\"\n",
    "    \n",
    "def evaluate(output, target):\n",
    "    mse = torch.mean((output - target) ** 2, dim=1)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mape = (\n",
    "        torch.mean(torch.abs(output - target) / (torch.abs(target) + 1), dim=1)\n",
    "    )\n",
    "    return [\n",
    "            (len(mse), torch.sum(mse).item()),\n",
    "            (len(rmse), torch.sum(rmse).item()),\n",
    "            (len(mape), torch.sum(mape).item()),\n",
    "        ]\n",
    "\n",
    "from scipy.sparse import coo_array\n",
    "\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "data = PeMS04('ICML2022Code/src/PeMS04/', aug_minutes=True, aug_weekdays=True)\n",
    "A_list = []\n",
    "N = data.raw_nodes.shape[1]\n",
    "X = data.raw_nodes\n",
    "# X_list = [torch.tensor(x).view(-1,1).to(device) for x in X]\n",
    "# for src, dst, weight in zip(data.edge_srcs, data.edge_dsts, data.edge_feats):\n",
    "srcs = data.edge_srcs\n",
    "dsts = data.edge_dsts\n",
    "src = np.concatenate([srcs, dsts])\n",
    "dst = np.concatenate([dsts, srcs])\n",
    "weight = np.concatenate([data.edge_feats, data.edge_feats])\n",
    "adj = aug_normalized_adjacency(coo_array((weight, (src, dst)), shape=(N,N)))\n",
    "A = sparse_mx_to_torch_sparse_tensor(adj).to(device)\n",
    "A_rho = get_spectral_rad(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217d9a13-9a8b-4c1c-af13-ab56448f19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bimodel(nn.Module):\n",
    "    def __init__(self, num_in, num_hid, num_out, num_node, time_steps, kappa=0.99, phi=F.relu, b_direct=False):\n",
    "        super(bimodel, self).__init__()\n",
    "        self.i = num_in\n",
    "        self.h = num_hid\n",
    "        self.o = num_out\n",
    "        self.n = num_node\n",
    "        self.t = time_steps\n",
    "        self.k = kappa\n",
    "        self.direct = b_direct\n",
    "\n",
    "        self.phi = [F.relu]*self.t\n",
    "        self.X_0 = Parameter(torch.zeros(self.h, num_node), requires_grad=False)\n",
    "        self.W = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.h)) for i in range(self.t)])\n",
    "#         self.W = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.h))])\n",
    "#         self.Omega = nn.ParameterList([Parameter(torch.FloatTensor(self.h, self.i)) for i in range(self.t)])\n",
    "        self.V = Parameter(torch.FloatTensor(self.h, self.i))\n",
    "#         self.linear = nn.Linear(self.h, self.o)\n",
    "        self.classifier = nn.Sequential(\n",
    "#                                     nn.Dropout(p=0.3)\n",
    "                                    nn.Linear(self.h, self.h),\n",
    "#                                     nn.BatchNorm1d(self.h),\n",
    "                                    nn.Softplus(),\n",
    "                                    nn.Linear(self.h, self.o),\n",
    "                                    # nn.LogSoftmax(dim=1)\n",
    "                                    )\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "#         stdv = 0.01\n",
    "        for i in range(len(self.W)):\n",
    "            stdv = 1. / (math.sqrt(self.W[i].size(1)))\n",
    "            self.W[i].data.uniform_(-stdv, stdv)\n",
    "#             self.Omega[i].data.uniform_(-stdv, stdv)\n",
    "#         stdv = 1. / self.W[0].size(1)\n",
    "#         self.W[0].data.uniform_(-stdv, stdv)\n",
    "        stdv = 1. /self.V.size(1)\n",
    "        self.V.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def project(self,X_list, A_list, A_rho):\n",
    "        \n",
    "        self.X_list = X_list\n",
    "        self.A_list = A_list\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i] = projection_norm_inf(self.W[i], kappa=self.k / A_rho[i])\n",
    "            \n",
    "        \n",
    "    def forward(self,Z):\n",
    "\n",
    "        X_list = self.X_list\n",
    "        A_list = self.A_list\n",
    "        V = self.V\n",
    "        W_list = self.W\n",
    "        phi = F.relu\n",
    "\n",
    "        \n",
    "        for j in range(len(A_list)):\n",
    "                W = W_list[j]\n",
    "                A = A_list[j]\n",
    "                B = torch.spmm(V, X_list[j])\n",
    "                Z_ = W @ Z\n",
    "                support = torch.spmm(A, Z_.T).T\n",
    "\n",
    "                Z = phi(support + B)\n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X_list, A_list, A_rho):\n",
    "        # for i in range(len(self.W)):\n",
    "        #     self.W[i] = projection_norm_inf(self.W[i], kappa=self.k / A_rho[i])\n",
    "        \n",
    "        V = self.V\n",
    "        W_list = self.W\n",
    "        phi = F.relu\n",
    "        max_iter = 300\n",
    "        tol = 1e-6\n",
    "        \n",
    "        device = W_list[0].device\n",
    "        Z = torch.zeros(self.X_0.shape).to(device)\n",
    "        \n",
    "        status = 'max itrs reached'\n",
    "        for i in range(max_iter):\n",
    "            Z_old = Z\n",
    "            for j in range(len(A_list)):\n",
    "                    W = W_list[j]\n",
    "                    A = A_list[j]\n",
    "                    B = torch.spmm(V, X_list[j])\n",
    "                    Z_ = W @ Z_old\n",
    "                    support = torch.spmm(A, Z_.T).T\n",
    "\n",
    "                    Z = phi(support + B)\n",
    "            \n",
    "            err = torch.norm(Z - Z_old, np.inf)\n",
    "            if err < tol:\n",
    "                status = 'converged'\n",
    "                break\n",
    "\n",
    "        if status == 'max itrs reached':\n",
    "                \n",
    "                print('Forward Not Converge! Error: %3.5f, tol: %3.5f' % (err, tol))\n",
    "            \n",
    "        Y_hat = self.classifier(Z.T)\n",
    "        return Y_hat\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee2d763-fa7c-410f-920c-31cdcf1628ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [10:10<00:00, 19.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.000291\n",
      "mse: 0.0010, mape: 0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [09:49<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 0.000201\n",
      "mse: 0.0004, mape: 0.0083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [09:46<00:00, 20.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 loss 0.000193\n",
      "mse: 0.0002, mape: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [10:23<00:00, 19.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 loss 0.000192\n",
      "mse: 0.0003, mape: 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [09:39<00:00, 20.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 loss 0.000191\n",
      "mse: 0.0002, mape: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [09:44<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 loss 0.000190\n",
      "mse: 0.0002, mape: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [09:42<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 loss 0.000190\n",
      "mse: 0.0002, mape: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [09:43<00:00, 20.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 loss 0.000190\n",
      "mse: 0.0003, mape: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 11886/11886 [09:44<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 loss 0.000189\n",
      "mse: 0.0003, mape: 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▏                                                                               | 988/11886 [00:48<08:54, 20.38it/s]"
     ]
    }
   ],
   "source": [
    "# Inductive ##########################################\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "idx = list(range(16980))\n",
    "train_indices = [idx[i*10:(i*10+7)] for i in range(1698)]\n",
    "# train_indices.append(idx[17840:])\n",
    "train_indices = list(itertools.chain.from_iterable(train_indices))\n",
    "\n",
    "val_indices = [idx[i*10+7:(i*10+8)] for i in range(1698)]\n",
    "val_indices = list(itertools.chain.from_iterable(val_indices))\n",
    "\n",
    "test_indices = [idx[i*10+8:(i+1)*10] for i in range(1698)]\n",
    "test_indices = list(itertools.chain.from_iterable(test_indices))\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_min = np.min(X_train)\n",
    "X_max = np.max(X_train)\n",
    "X_new = (X-X_min)/(X_max-X_min)\n",
    "X_list = [torch.tensor(x.T, dtype=torch.float).to(device) for x in X_new]\n",
    "\n",
    "\n",
    "###################################### Train ############################################\n",
    "bests = []\n",
    "for j in range(5):\n",
    "    model = bimodel(5, 16,3, 307, 12, kappa=0.99).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                       lr=0.01, weight_decay=1e-5)\n",
    "    \n",
    "    Z_all = [torch.zeros(16*307)]*len(idx)\n",
    "    V_all = [torch.zeros(16*307)]*len(idx)\n",
    "    eta_1 = 0.9\n",
    "    eta_2 = 0.001\n",
    "    \n",
    "    for i in range(10):\n",
    "        losses = []\n",
    "        random.shuffle(train_indices)\n",
    "        model.train()\n",
    "        with tqdm(train_indices) as tq:\n",
    "            for idx in tq:\n",
    "                A_list_tmp = [A]*12\n",
    "                A_rho_tmp = [A_rho]*12\n",
    "                X_list_tmp = X_list[idx:idx+12]\n",
    "                y = X_list[idx+12][:3].T\n",
    "\n",
    "                \n",
    "\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "\n",
    "                model.project(X_list_tmp, A_list_tmp, A_rho_tmp)\n",
    "                \n",
    "                for k in range(1):\n",
    "                    \n",
    "                    Z_0 = Z_all[idx]\n",
    "                    V_0 = V_all[idx]\n",
    "                    \n",
    "                    Z_1 = Z_0.to(device).requires_grad_(True)\n",
    "                    Z = model(Z_1.view(16, -1))\n",
    "\n",
    "                    Y_hat = model.classifier(Z.T)\n",
    "                    loss = torch.square(Y_hat-y).mean()\n",
    "                    loss2 = torch.norm(Z_1-Z.reshape(-1), 2)\n",
    "\n",
    "                    z_grad = torch.autograd.grad(loss, Z_1, retain_graph=True)\n",
    "\n",
    "                    Z_0 = (1 - eta_1)*Z_0 + eta_1*Z.reshape(-1).detach().cpu()\n",
    "\n",
    "                    g_z_grad = torch.autograd.grad(loss2, Z_1, retain_graph=True, create_graph=True)\n",
    "\n",
    "                    hv = torch.inner(g_z_grad[0],V_0.to(device))\n",
    "                    phi_v = torch.autograd.grad(hv, Z_1, retain_graph=True)\n",
    "\n",
    "                    V_0 = V_0 - eta_2*phi_v[0].cpu() + eta_2*z_grad[0].cpu()\n",
    "                    # V_0 = V_0 - eta_2*V_0 + eta_2*z_grad[0].cpu()\n",
    "\n",
    "                    loss3 = -torch.inner(g_z_grad[0],V_0.to(device))\n",
    "                    loss3.backward(retain_graph=True)\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    Z_all[idx] = Z_0\n",
    "                    V_all[idx] = V_0\n",
    "                    # import ipdb; ipdb.set_trace()\n",
    "                losses.append(loss.item())\n",
    "            print('epoch',i,f'loss {np.mean(losses) :.6f}')\n",
    "\n",
    "####################################### Test ##############################################\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            model.eval()\n",
    "            for idx in test_indices:\n",
    "                A_list_tmp = [A]*12\n",
    "                A_rho_tmp = [A_rho]*12\n",
    "                X_list_tmp = X_list[idx:idx+12]\n",
    "                y = X_list[idx+12][:3].T\n",
    "\n",
    "                # model.train()\n",
    "                with torch.no_grad():\n",
    "                    out_Z = model.predict(X_list_tmp, A_list_tmp, A_rho_tmp)\n",
    "                # import ipdb; ipdb.set_trace()\n",
    "                y_pred.append(out_Z.detach())\n",
    "                y_true.append(y)\n",
    "\n",
    "            y_true = torch.concat(y_true, 0)\n",
    "            y_pred = torch.concat(y_pred, 0)\n",
    "            results = evaluate(y_pred, y_true)\n",
    "            mse = results[0][1]/results[0][0]\n",
    "            mape = results[2][1]/results[2][0]\n",
    "            print(f'mse: {mse:.4f}, mape: {mape:.4f}')\n",
    "        bests.append(mape)\n",
    "    \n",
    "print(np.mean(bests), np.std(bests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbdc4e7-c0a3-41d2-a83c-002cfbd9c513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
